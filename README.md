<div align="center">

![Spark Analyzer Logo](docs/images/logo.svg)


--- 
**Apache Spark ë¦¬ì†ŒìŠ¤ íŠœë‹ì„ ìœ„í•œ Spark ì´ë²¤íŠ¸ ë¡œê·¸ ë¶„ì„ê¸°**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
</div>


`Spark Analyzer`ëŠ” Spark History Serverì—ì„œ ë‹¤ìš´ë¡œë“œ ë°›ì€ ì´ë²¤íŠ¸ ë¡œê·¸ ë¶„ì„ ê²°ê³¼ë¥¼ ì›¹ ê¸°ë°˜ ëŒ€ì‹œë³´ë“œë¥¼ í†µí•´ Spill, Skew, Shuffle Load ë“± 30ê°œ ì´ìƒì˜ ì„±ëŠ¥ ì§€í‘œë¡œ ê³„ì‚°í•˜ì—¬ ë³‘ëª© ì§€ì ì„ ì‹ë³„í•˜ê³  ì´ë¥¼ ë°ì´í„° ì—”ì§€ë‹ˆì–´ê°€ íš¨ìœ¨ì ìœ¼ë¡œ ìµœì í™”í•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤.

---

<br>

## âœ¨ ì£¼ìš” ê¸°ëŠ¥ (Key Features)

### ğŸ“‚ ì´ë²¤íŠ¸ ë¡œê·¸ ê´€ë¦¬
- **ê°„í¸í•œ ì—…ë¡œë“œ**: ë“œë˜ê·¸ ì•¤ ë“œë¡­ ë˜ëŠ” íŒŒì¼ ì„ íƒìœ¼ë¡œ Spark History Serverì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ì´ë²¤íŠ¸ ë¡œê·¸ ì—…ë¡œë“œ
- **ìë™ ì¸ì‹**: ì••ì¶• íŒŒì¼(`.zstd`) ë° ë¡¤ë§ ë¡œê·¸(`events_1_`, `events_2_`) ìë™ ì²˜ë¦¬
- **ë¡œê·¸ ê´€ë¦¬**: ì—…ë¡œë“œëœ ë¡œê·¸ ëª©ë¡ ì¡°íšŒ, ì„ íƒ ì‚­ì œ, ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ë¦„ ìë™ ì¶”ì¶œ

### ğŸ“Š ì´ì¤‘ ë¶„ì„ ëª¨ë“œ
- **ìš”ì•½ ë¶„ì„ (Summary Analysis)**: 
  - ì—¬ëŸ¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ í•œ ë²ˆì— ì„ íƒí•˜ì—¬ 30ê°œ ì´ìƒì˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¹„êµ ë¶„ì„
  - Duration, CPU/Memory ì‚¬ìš©ë¥ , Shuffle, Spill, I/O ë“± í•µì‹¬ ì§€í‘œ ì œê³µ
  - ì‹œê³„ì—´ ê·¸ë˜í”„ë¡œ ë©”íŠ¸ë¦­ íŠ¸ë Œë“œ ì‹œê°í™”
  
- **ìƒì„¸ ë¶„ì„ (Detail View)**:
  - ë‹¨ì¼ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ Stageë³„ ìƒì„¸ ë©”íŠ¸ë¦­ ë¶„ì„
  - Read/Write ë°ì´í„° íë¦„ ì‹œê°í™” (íŒŒì¼ ê²½ë¡œ, í¬ë§·, íŒŒí‹°ì…˜ ìˆ˜ ë“±)
  - Executor íƒ€ì„ë¼ì¸ ì°¨íŠ¸ë¡œ ë¦¬ì†ŒìŠ¤ í™œìš© íŒ¨í„´ í™•ì¸

### ğŸ¯ 30+ ì„±ëŠ¥ ë©”íŠ¸ë¦­
ë¦¬ì†ŒìŠ¤ ìµœì í™”ë¥¼ ìœ„í•œ í•µì‹¬ ì§€í‘œ ì œê³µ:
- **ë¦¬ì†ŒìŠ¤**: Driver/Executor Cores/Memory, Overhead, ìµœëŒ€ í™œì„± Executor ìˆ˜
- **íš¨ìœ¨ì„±**: Avg Idle Cores (%), Peak Memory Usage (%)
- **I/O**: Total Input/Output, Write Partitions (Files)
- **Shuffle**: Total/Max Shuffle Read/Write (Stage/Jobë³„)
- **ë³‘ëª© ì§€í‘œ**: Total/Max Spill (Memory/Disk), Spill ë°œìƒ Stage ID
- **ì•ˆì •ì„±**: Preempted Tasks/Executors

### ğŸ› ï¸ ë¶„ì„ ë„êµ¬
- **CSV ë‚´ë³´ë‚´ê¸°**: ë¶„ì„ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì¶”ê°€ ë¶„ì„ ê°€ëŠ¥
- **ë‹¨ìœ„ ë³€í™˜**: B, KB, MB, GB, TB ë‹¨ìœ„ ê°„ ì‹¤ì‹œê°„ ì „í™˜
- **ë™ì  ì •ë ¬**: ëª¨ë“  ì»¬ëŸ¼ í´ë¦­ìœ¼ë¡œ ì˜¤ë¦„ì°¨ìˆœ/ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
- **ì»¬ëŸ¼ ì¬ë°°ì¹˜**: ë“œë˜ê·¸ ì•¤ ë“œë¡­ìœ¼ë¡œ í…Œì´ë¸” ì»¬ëŸ¼ ìˆœì„œ ë³€ê²½
- **ë©”íŠ¸ë¦­ íˆ´íŒ**: ê° ë©”íŠ¸ë¦­ì— ë§ˆìš°ìŠ¤ ì˜¤ë²„ ì‹œ ìƒì„¸ ì„¤ëª… í‘œì‹œ
- **SHS ì—°ë™**: Application ID í´ë¦­ ì‹œ Spark History Server í˜ì´ì§€ë¡œ ë°”ë¡œ ì´ë™

<br>

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ (Quick Start)

### ì‚¬ì „ ìš”êµ¬ ì‚¬í•­ (Prerequisites)
- **Python 3.8+** (`pyenv` ì‚¬ìš© ê¶Œì¥ - 3.11)
- ìµœì‹  ì›¹ ë¸Œë¼ìš°ì € (Chrome, Edge, Safari)

### ì„¤ì¹˜ ë° ì‹¤í–‰ (Installation & Run)
í¸ë¦¬í•œ `Makefile`ì„ ì œê³µí•˜ì—¬ ëª‡ ì´ˆ ë§Œì— ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### 1. **Pyenv ì„¤ì¹˜**
   
   macOSì—ì„œ Homebrewë¥¼ ì‚¬ìš©í•˜ì—¬ pyenvë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤:
   ```bash
   $ brew install pyenv pyenv-virtualenv
   ```

   ì„¤ì¹˜ í›„ ì…¸ ì„¤ì • íŒŒì¼ì— pyenvë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤ (`.zshrc` ë˜ëŠ” `.bash_profile`):
   ```bash
   $ echo 'export PYENV_ROOT="$HOME/.pyenv"' >> ~/.zshrc
   $ echo 'export PATH="$PYENV_ROOT/bin:$PATH"' >> ~/.zshrc
   $ echo 'eval "$(pyenv init --path)"' >> ~/.zshrc
   $ echo 'eval "$(pyenv init -)"' >> ~/.zshrc
   $ echo 'eval "$(pyenv virtualenv-init -)"' >> ~/.zshrc
   ```

   ì…¸ì„ ì¬ì‹œì‘í•˜ê±°ë‚˜ ì„¤ì •ì„ ë‹¤ì‹œ ë¡œë“œí•©ë‹ˆë‹¤:
   ```bash
   $ source ~/.zshrc
   ```

#### 2. **Python 3.11 ì„¤ì¹˜**
   
   pyenvë¥¼ ì‚¬ìš©í•˜ì—¬ Python 3.11ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤:
   ```bash
   $ pyenv install 3.11
   ```

   ì„¤ì¹˜ ê°€ëŠ¥í•œ Python ë²„ì „ í™•ì¸:
   ```bash
   $ pyenv install --list | grep 3.11
   ```

#### 3. **ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”**
   
   Python 3.11 ê¸°ë°˜ì˜ `spark_analyzer_env` ê°€ìƒí™˜ê²½ì„ ìƒì„±í•©ë‹ˆë‹¤:
   ```bash
   $ pyenv virtualenv 3.11 spark_analyzer_env
   ```

   í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™í•˜ì—¬ ê°€ìƒí™˜ê²½ì„ í™œì„±í™”í•©ë‹ˆë‹¤:
   ```bash
   $ cd /path/to/spark_analyzer
   $ pyenv activate spark_analyzer_env
   ```

   ë˜ëŠ” í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ ìë™ìœ¼ë¡œ í™œì„±í™”ë˜ë„ë¡ ì„¤ì •:
   ```bash
   $ pyenv local spark_analyzer_env
   ```

#### 4. **ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ì„¤ì¹˜**
   
   ê°€ìƒí™˜ê²½ì´ í™œì„±í™”ëœ ìƒíƒœì—ì„œ `requirements.txt`ì˜ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤:
   ```bash
   (spark_analyzer_env) $ pip install -r requirements.txt
   ```

   ì„¤ì¹˜ í™•ì¸:
   ```bash
   (spark_analyzer_env) $ pip list
   ```

#### 5. **ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰**
   `SHS_URL` í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì—¬ Spark History Serverì™€ ì—°ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ê¸°ë³¸ê°’: `http://localhost:18080`)

   ```bash
   # í¬ê·¸ë¼ìš´ë“œ (ê°œë°œìš©)
   (spark_analyzer_env) $ make run

   # ë°±ê·¸ë¼ìš´ë“œ (ì„œë¹„ìŠ¤ìš©)
   (spark_analyzer_env) $ export SHS_URL="http://spark-history-server:18080"
   (spark_analyzer_env) $ make start

   # ì‚¬ìš©ì ì •ì˜ SHS URL ì‚¬ìš© ì˜ˆì‹œ
   (spark_analyzer_env) $ export SHS_URL="http://spark-history-server:18080"
   (spark_analyzer_env) $ make run
   ```

   > **SHS ì—°ë™**: í¸ì˜ë¥¼ ìœ„í•´ í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ë©´, ë¶„ì„ ê²°ê³¼ì˜ **Application ID**ë¥¼ í´ë¦­í–ˆì„ ë•Œ í•´ë‹¹ Spark History Server í˜ì´ì§€ë¡œ ë°”ë¡œ ì´ë™í•©ë‹ˆë‹¤.

#### 6. **ëŒ€ì‹œë³´ë“œ ì ‘ì†**
   ë¸Œë¼ìš°ì €ì—ì„œ [http://localhost:8000](http://localhost:8000)ì„ ì—½ë‹ˆë‹¤.

> **ì°¸ê³ **: 
> - ë°±ê·¸ë¼ìš´ë“œ ì„œë¹„ìŠ¤ë¥¼ ì¤‘ì§€í•˜ë ¤ë©´ `make stop`ì„ ì‹¤í–‰í•˜ì„¸ìš”.
> - í™˜ê²½ì„ ì´ˆê¸°í™”í•˜ë ¤ë©´ `make clean`ì„ ì‚¬ìš©í•˜ì„¸ìš”.
> - ê°€ìƒí™˜ê²½ì„ ë¹„í™œì„±í™”í•˜ë ¤ë©´ `pyenv deactivate`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.

<br>

## Spark History Server Event Log ë‹¤ìš´ë¡œë“œ ë° ì—…ë¡œë“œ

* Spark History Serverì—ì„œ ë¶„ì„í•  Applicationì˜ Event Logë¥¼ ë‹¤ìš´ë¡œë“œ ë°›ì€ í›„ ì•„ë˜ `Unpload log files` ì²˜ëŸ¼ Event Log íŒŒì¼ì„ `Spark Analyzer`ì— ì—…ë¡œë“œ í•˜ì„¸ìš”.

<img src="docs/images/spark_history_server_demo.png" alt="Upload Log Files" width="100%">

<br>

## ğŸ“¸ Snapshots
### Upload log files

<img src="docs/images/spark_analyzer_file_upload.gif" alt="Upload Log Files" width="100%">

### Analyze Seleted

<img src="docs/images/spark_analyzer_analyze_selected.gif" alt="Analyze Selected" width="100%">

### Create Graph

<img src="docs/images/spark_analyzer_create_graph.gif" alt="Create Graph" width="100%">

### Detail View

<img src="docs/images/spark_analyzer_detail_view.gif" alt="Detail View" width="100%">


## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡° (Project Structure)

```text
spark_analyzer/
â”œâ”€â”€ Makefile                    # ì‹¤í–‰ ê´€ë¦¬
â”œâ”€â”€ requirements.txt            # ì˜ì¡´ì„± ëª©ë¡
â”œâ”€â”€ web_app.py                  # FastAPI ì„œë²„
â”œâ”€â”€ spark_log_parser.py         # ë¡œê·¸ íŒŒì‹± ì—”ì§„
â”œâ”€â”€ spark_metric_definitions.json # ë©”íŠ¸ë¦­ ì •ì˜
â”œâ”€â”€ event_logs/                 # ë¡œê·¸ ì €ì¥ì†Œ
â”œâ”€â”€ latest_analysis_result.csv  # ë¶„ì„ ìºì‹œ
â””â”€â”€ static/                     # í”„ë¡ íŠ¸ì—”ë“œ ë¦¬ì†ŒìŠ¤
    â”œâ”€â”€ index.html              # SPA ì§„ì…ì 
    â”œâ”€â”€ css/style.css           # ëª¨ë˜ ìŠ¤íƒ€ì¼ë§
    â””â”€â”€ js/app.js               # ì•± ë¡œì§
```

<br>

## ğŸ¯ ë¦¬ì†ŒìŠ¤ ìµœì í™” ë°©ë²•ë¡  ìš”ì•½(Resource Optimization Methodology Summary)

> ìƒì„¸ ë¬¸ì„œ: https://velog.io/@todaybow/spark-executor-resource-tuning-guide

Spark Analyzerë¥¼ í™œìš©í•˜ì—¬ **Spark History Server**ì—ì„œ ì›í•˜ëŠ” ê¸°ê°„ì˜ Event Logë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³ , ë©”íŠ¸ë¦­ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì• í”Œë¦¬ì¼€ì´ì…˜ ì„±ê²©ì— ë§ëŠ” ë¦¬ì†ŒìŠ¤ ìµœì í™”ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ìµœì í™” ì›Œí¬í”Œë¡œìš°

1. **Event Log ìˆ˜ì§‘**: Spark History Serverì—ì„œ ë¶„ì„í•˜ë ¤ëŠ” ê¸°ê°„ì˜ Event Log ë‹¤ìš´ë¡œë“œ
2. **ë©”íŠ¸ë¦­ ë¶„ì„**: Spark Analyzerë¡œ ì—…ë¡œë“œí•˜ì—¬ Duration, Idle Cores, Peak Memory, Spill, Shuffle ë“± ì£¼ìš” ë©”íŠ¸ë¦­ í™•ì¸
3. **ì „ëµ ì„ íƒ**: ì• í”Œë¦¬ì¼€ì´ì…˜ íŠ¹ì„±ì— ë”°ë¼ Static ë˜ëŠ” Dynamic Allocation ì „ëµ ì ìš©
4. **ë¦¬ì†ŒìŠ¤ ì‚°ì •**: ë¶„ì„ëœ ë©”íŠ¸ë¦­ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ Executor ìˆ˜, ë©”ëª¨ë¦¬, ì½”ì–´ ìˆ˜ ê³„ì‚°

### ì£¼ìš” ë¶„ì„ ë©”íŠ¸ë¦­

Spark AnalyzerëŠ” ë‹¤ìŒê³¼ ê°™ì€ í•µì‹¬ ë©”íŠ¸ë¦­ì„ ì œê³µí•˜ì—¬ ë¦¬ì†ŒìŠ¤ ìµœì í™”ë¥¼ ì§€ì›í•©ë‹ˆë‹¤:

| ë©”íŠ¸ë¦­ | ì„¤ëª… | ìµœì í™” íŒíŠ¸ |
|:---|:---|:---|
| **Duration** | ì „ì²´ ì‘ì—… ì‹¤í–‰ ì‹œê°„ | ë³‘ë ¬ì„± ì¡°ì •ìœ¼ë¡œ ë‹¨ì¶• ê°€ëŠ¥ |
| **Idle Cores (%)** | ìœ íœ´ ì½”ì–´ ë¹„ìœ¨ | ë†’ì„ìˆ˜ë¡ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ â†’ ì½”ì–´ ìˆ˜ ê°ì†Œ ê³ ë ¤ |
| **Peak Memory (%)** | ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  | 90% ì´ìƒ ì‹œ ë©”ëª¨ë¦¬ ì¦ì„¤ í•„ìš” |
| **Total/Max Spill (Disk)** | ë””ìŠ¤í¬ ìŠ¤í•„ ë°œìƒëŸ‰ | ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹ í˜¸ â†’ ë©”ëª¨ë¦¬ ì¦ì„¤ ë˜ëŠ” íŒŒí‹°ì…˜ ì¡°ì • |
| **Max Shuffle Read/Write** | ìµœëŒ€ ì…”í”Œ ë°ì´í„° í¬ê¸° | íŒŒí‹°ì…˜ ìˆ˜ ë° ë©”ëª¨ë¦¬ ì‚°ì •ì˜ ê¸°ì¤€ |

### ğŸš€ ì „ëµ 1: Static Allocation (ê· ë“±í•œ ë°ì´í„° íŒ¨í„´)

**ì ìš© ëŒ€ìƒ**: Input/Output/Shuffle í¬ê¸°ê°€ ì¼ì •í•˜ê³  ë°ì´í„° ë¶„í¬ê°€ ê³ ë¥¸ ë°°ì¹˜ ì‘ì—…

**í•µì‹¬ ì„¤ì •**:
- **Partition Size**: 128MB (HDFS I/O ìµœì í™”)
- **Executor Spec**: 4 cores / 6GB memory
- **Waves**: 3-10íšŒ (ë³‘ë ¬ ì²˜ë¦¬ ë°°ìˆ˜)
- **Safety Factor**: 3-5

**ì‚°ì • ì˜ˆì‹œ** (Max Shuffle 1TB ê¸°ì¤€):
```
1. Shuffle Partitions = 1TB / 128MB = 8,192ê°œ
2. Total Cores = 8,192 / 10 waves = 820 cores
3. Executor Instances = 820 / 4 = 205ê°œ
4. Executor Memory = (128MB * 3 * 4 * 2 / 0.6) + 300MB â‰ˆ 6GB
```

> **ğŸ’¡ Tip**: Spark Analyzerì˜ **Idle Cores** ì§€í‘œê°€ ë†’ë‹¤ë©´ Executor ìˆ˜ë¥¼ ì¤„ì´ê³ , **Spill** ë°œìƒ ì‹œ ë©”ëª¨ë¦¬ë¥¼ ì¦ì„¤í•˜ì„¸ìš”.

### ğŸš€ ì „ëµ 2: Dynamic Allocation (ë¶ˆê· ë“±í•œ ë°ì´í„° íŒ¨í„´)

**ì ìš© ëŒ€ìƒ**: Join/Explode/Inlineìœ¼ë¡œ ì¤‘ê°„ ì…”í”Œ ê¸‰ì¦, ì‹¤í–‰ë§ˆë‹¤ ë°ì´í„° í¸ì°¨ê°€ í° ì‘ì—…

**í•µì‹¬ ì„¤ì •**:
- **Partition Size**: 64MB (Data Skew ì €í•­ì„± ê°•í™”)
- **Executor Spec**: 4 cores / 10GB memory
- **Safety Factor**: 3-20 (Skew ê³ ë ¤)
- **Dynamic Config**: `spark.dynamicAllocation.enabled=true`

**ì‚°ì • ì˜ˆì‹œ** (Max Shuffle 1TB ê¸°ì¤€):
```
1. Shuffle Partitions = 1TB / 64MB = 16,384ê°œ
2. Total Cores = 16,384 / 10 waves = 1,639 cores
3. Max Executors = 1,639 / 4 = 410ê°œ
4. Executor Memory = (64MB * 10 * 4 * 2 / 0.6) + 300MB â‰ˆ 10GB
```

> **âš ï¸ Warning**: **Max Spill (Disk)** ê°’ì´ í¬ê±°ë‚˜ íŠ¹ì • Stageì—ì„œ Task Durationì´ ë¶ˆê· ë“±í•˜ë‹¤ë©´ Dynamic Allocation + ì‘ì€ íŒŒí‹°ì…˜ ì „ëµì„ ì ìš©í•˜ì„¸ìš”.

### ì‹¤ì „ í™œìš© ê°€ì´ë“œ

1. **ë©”íŠ¸ë¦­ ìˆ˜ì§‘**: ìµœì†Œ 1ì£¼ì¼ ì´ìƒì˜ Event Logë¥¼ Spark Analyzerë¡œ ë¶„ì„
2. **íŒ¨í„´ íŒŒì•…**: 
   - Shuffle í¬ê¸°ê°€ ì¼ì • â†’ Static Allocation
   - Shuffle í¬ê¸° ë³€ë™ í° ê²½ìš° â†’ Dynamic Allocation
3. **ë¦¬ì†ŒìŠ¤ ê³„ì‚°**: ìœ„ ê³µì‹ì„ í™œìš©í•˜ì—¬ Executor ìˆ˜, ë©”ëª¨ë¦¬ ì‚°ì •
4. **ê²€ì¦ ë° íŠœë‹**: ì ìš© í›„ ë‹¤ì‹œ ë¶„ì„í•˜ì—¬ Idle Cores, Spill ê°œì„  í™•ì¸

### Spark Executor ë©”ëª¨ë¦¬ êµ¬ì£¼ë³„ ìƒì„¸ ê³„ì‚°í‘œ(Spec Sheet)
**(spark.memory.fraction=0.6, spark.executor.memoryOverheadFactor=0.1 ê¸°ì¤€)**

| Memory (Heap) | Overhead (Min 384MB) | Unified Total | Execution / Storage (5:5) | ë¹„ê³  |
| :--- | :--- | :--- | :--- | :--- |
| **2g** | 384 MB | 1,049 MB | 524 MB / 524 MB | ì†Œê·œëª¨ ì‘ì—…ìš© |
| **4g** | 410 MB | 2,278 MB | 1,139 MB / 1,139 MB | |
| **6g** | 614 MB | 3,506 MB | 1,753 MB / 1,753 MB | **Static ê¶Œì¥** |
| **8g** | 819 MB | 4,735 MB | 2,368 MB / 2,368 MB | |
| **10g** | 1,024 MB | 5,964 MB | 2,982 MB / 2,982 MB | **Dynamic ê¶Œì¥** |
| **12g** | 1,229 MB | 7,193 MB | 3,596 MB / 3,596 MB | |
| **16g** | 1,638 MB | 9,650 MB | 4,825 MB / 4,825 MB | |
| **20g** | 2,048 MB | 12,108 MB | 6,054 MB / 6,054 MB | |
| **24g** | 2,458 MB | 14,565 MB | 7,283 MB / 7,283 MB | ëŒ€ê·œëª¨ ì§‘ê³„ìš© |
| **30g** | 3,072 MB | 18,252 MB | 9,126 MB / 9,126 MB | ëŒ€ê·œëª¨ ì§‘ê³„ìš© |
| **36g** | 3,686 MB | 21,938 MB | 10,969 MB / 10,969 MB | ëŒ€ê·œëª¨ ì§‘ê³„ìš© |

### ì°¸ê³  ìë£Œ

ë¦¬ì†ŒìŠ¤ ì‚°ì •ì— ëŒ€í•œ ìƒì„¸í•œ ë°°ê²½ ì§€ì‹ê³¼ ê³„ì‚° ê³µì‹ì€ ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”:
- [Spark Executor ë¦¬ì†ŒìŠ¤ íŠœë‹ ê°€ì´ë“œ(Static vs Dynamic Allocation)](https://velog.io/@todaybow/spark-executor-resource-tuning-guide)
- [Spark Executor Memory êµ¬ì¡° ë° Shuffle Spill ë¶„ì„](https://velog.io/@todaybow/spark-executor-memory-architectures)
- Spark ê³µì‹ ë¬¸ì„œ: [Monitoring and Instrumentation](https://spark.apache.org/docs/latest/monitoring.html)
